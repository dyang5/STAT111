\newpage
\setcounter{section}{5}
\section{Tests of Hypotheses}
\subsection{GLR Test for Multinomial Distribution}

\begin{enumerate}[a)]
    \item For $X_j$ the count of dice rolls equal to $j$ (for $j \in \{ 1, \dots, 6 \}$), the joint likelihood function for the $L(\theta_1, \dots, \theta_6)$ is
    \[
        L(\theta_1, \dots, \theta_6) = \frac{n!}{x_1! \dots x_6!}\theta_1^{x_1} \dots \theta_6^{x_6}.
    \]
    Equivalently, the log-likelihood is
    \[
        l(\theta_1, \dots, \theta_6) = \log \left(\frac{n!}{x_1! \dots x_6!}\right) + \sum_{i=1}^6 x_i \log(\theta_i)
    \]
    where $\sum \theta_i = 1$. \\

    To determine the MLE's $\hat{\theta}_i$, we use Lagrange Multipliers and work to maximize
    \[
        l(\theta_1, \dots, \theta_6, \lambda) = \log \left(\frac{n!}{x_1! \dots x_6!}\right) + \sum_{i=1}^6 x_i \log(\theta_i) + \lambda\left(\sum_{i=1}^6 \theta_i - 1\right)
    \]
    where the extra $\lambda(\sum_{i=1}^6 \theta_i - 1)$ arises from the Lagrange multiplier $\lambda$ and the constraint $\sum \theta_i = 1$. To maximize $l(\theta_1, \dots, \theta_6, \lambda)$, we need the partials with respect
    to each $\theta_i$ and $\lambda$ to be equal to $0$:
    \[
        \frac{\partial l}{\partial \theta_i} = \frac{x_i}{\theta_i} + \lambda = 0 \text{ and } \frac{\partial l}{\partial \lambda} = \left(\sum \theta_i \right) - 1 = 0.
    \]
    The former condition $\frac{x_i}{\theta_i} + \lambda = 0$ tells us that $\hat{\theta}_i = -\frac{x_i}{\lambda}$. \\
    
    Now, the latter condition $\left(\sum \theta_i \right) - 1 = 0$ tells us that
    \[
        \sum \hat{\theta}_i = - \frac{\sum x_i}{\lambda} = 1,
    \]
    and solving for $\lambda$ gives us $\hat{\lambda} = - \sum x_i$. Thus, for each $\hat{\theta}_i$, we know that
    \[
        \hat{\theta}_i = -\frac{x_i}{\lambda} = \frac{-x_i}{- \sum x_i} = \frac{x_i}{n}
    \]    
    as desired.

    \item As we saw previously, 
    \[
        L(\theta_1, \dots, \theta_6) = \frac{n!}{x_1! \dots x_6!}\theta_1^{x_1} \dots \theta_6^{x_6}
    \]
    Our null hypothesis is $H_0\colon \theta_1 = \dots = \theta_6 = \frac{1}{6}$ and alternative hypothesis is $H_1\colon $ not $H_0$, so
    \begin{align*}
        \Lambda &= \frac{\max\limits_{H_0} \frac{n!}{x_1! \dots x_6!}\theta_1^{x_1} \dots \theta_6^{x_6}}{\max\limits_{H_0 \cup H_1} \frac{n!}{x_1! \dots x_6!}\theta_1^{x_1} \dots \theta_6^{x_6}} \\
        &= \frac{\left( \frac{1}{6} \right)^{x_1 + \dots + x_6} }{\left( \frac{x_1}{n} \right)^{x_1} \dots \left( \frac{x_6}{n} \right)^{x_6} }
    \end{align*}
    where the denominator is from the fact that $\hat{\theta}_i = \frac{x_i}{n}$, which we derived in part (a). Note that
    \begin{align*}
        -2 \log (\Lambda) &= -2 \sum_{i=1}^6 x_i \log \left( \frac{\frac{1}{6}}{\frac{x_i}{n}} \right) \\
        &= -2 \left( \sum x_i \log \frac{1}{6} - \sum x_i \log \left( \frac{x_i}{n} \right) \right) \\
        &= -2 \left( n \log \left(\frac{1}{6}\right) - \sum \log \left( \left( \frac{x_i}{n} \right)^{x_i} \right) \right) 
    \end{align*}

    The Chi-square approximation for $-2 \log \Lambda \dot\sim \chi^2_{\nu}$ where $\nu$ is the difference in the dimensions of the null and alternative parameter spaces; in this case, $\nu = 6 - 1 = 5$. \\

    For the given data, we find that 
    \[
        \Lambda = \frac{\left( \frac{1}{6} \right)^{24} }{\frac{8}{24}^8 \frac{4}{24}^{16}} = \frac{1}{256}
    \]
    so $-2 \log \Lambda \approx 11.1$. Using the Chi-Square approximation $\chi^2_5$, we find the associated p-value using the R command \textit{$1$ - pchisq($11.1, 5)$ = 0.049}.
    We can compare this approximation to the exact p-value which we can acquire through simulation (see attached R Code). \\

    Note that the interpretation of the p-value is the probability of achieving such an extreme value of $-2\log \Lambda$ in $n$ rolls, assuming $H_0$ is true.

    \item See the above image for the GLR test for independence of the row and column variables in a two-way table of counts.\\
    
    To show the asymptotic equivalence of the Chi-square approximation for $-2 \log \Lambda$ with the Pearson Chi-square test of independence, we will manipulate the expression for $-2 \log \Lambda$. \\

    
\end{enumerate}